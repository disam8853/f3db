{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_pipe = {\n",
    "    \"collaborator\": [\n",
    "        {\n",
    "            \"name\": \"StandardScaler\",\n",
    "            \"parameter\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SaveData\",\n",
    "            \"parameter\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"StandardScaler\",\n",
    "            \"parameter\": []\n",
    "        }\n",
    "    ],\n",
    "    \"global-server\": [\n",
    "        {\n",
    "            \"name\": \"StandardScaler\",\n",
    "            \"parameter\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"train_test_split\",\n",
    "            \"parameter\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MinMaxScaler\",\n",
    "            \"parameter\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SVC\",\n",
    "            \"parameter\": []\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fitted(clf): \n",
    "    X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "    y = np.array([1, 1, 2, 2])\n",
    "    print(clf,hasattr(clf, \"predict\"))\n",
    "    return hasattr(clf, \"predict\")\n",
    "def parse_global_pipeline(raw_pipe_data,character):\n",
    "    final_pipeline = []\n",
    "    param_pipeline = []\n",
    "    sub_pipeline = []\n",
    "    pipe = raw_pipe_data[character]\n",
    "    for idx in range(len(pipe)):\n",
    "        # print(pipe[idx])\n",
    "        if(pipe[idx]['name'] != 'SaveData' and pipe[idx]['name'] != 'train_test_split'):\n",
    "            strp = pipe[idx]['name']+'()'\n",
    "            if check_fitted(eval(strp)):\n",
    "                sub_pipeline.append(('model',eval(strp)))\n",
    "            else:\n",
    "                sub_pipeline.append((pipe[idx]['name'],eval(strp)))\n",
    "        elif(pipe[idx]['name'] == 'train_test_split'):\n",
    "            final_pipeline.append(sub_pipeline)\n",
    "            sub_pipeline = []\n",
    "            final_pipeline.append('train_test_split')\n",
    "        else:\n",
    "            final_pipeline.append(sub_pipeline)\n",
    "            final_pipeline.append(pipe[idx]['name'])\n",
    "            sub_pipeline = []      \n",
    "\n",
    "    final_pipeline.append(sub_pipeline)\n",
    "    return final_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler() False\n",
      "MinMaxScaler() False\n",
      "SVC() True\n",
      "[[('StandardScaler', StandardScaler())], 'train_test_split', [('MinMaxScaler', MinMaxScaler()), ('model', SVC())]]\n",
      "lll\n",
      "TT\n",
      "[('MinMaxScaler', MinMaxScaler()), ('model', SVC())]\n"
     ]
    }
   ],
   "source": [
    "def train_test_split_training(model_pipeline, src_id):\n",
    "    print(model_pipeline)\n",
    "\n",
    "parsed_pipeline = parse_global_pipeline(raw_pipe, \"global-server\")\n",
    "print(parsed_pipeline)\n",
    "\n",
    "for sub_pipeline_idx in range(len(parsed_pipeline)):\n",
    "    sub_pipeline = parsed_pipeline[sub_pipeline_idx]\n",
    "    \n",
    "    if(sub_pipeline == 'train_test_split'):\n",
    "        \n",
    "        src_id = train_test_split_training(parsed_pipeline[sub_pipeline_idx+1],src_id=1)\n",
    "        break\n",
    "    else:\n",
    "        # train_test_split 之前要儲存資料\n",
    "        sub_pipeline_param_list = pipe_param_string[parsed_pipeline.index(sub_pipeline)]\n",
    "        src_id = build_pipeline(dag, src_id, sub_pipeline, param_list=sub_pipeline_param_list, experiment_number=experiment_number)\n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_param(raw_pipe_data, character):\n",
    "    final_pipeline_param = []\n",
    "    param_pipeline = {}\n",
    "    # sub_pipeline = []\n",
    "    pipe = raw_pipe_data[character]\n",
    "    for idx in range(len(pipe)):\n",
    "        # print('yeeeeeeeeeeeeeeeeeeeeee',pipe[idx])\n",
    "        if(pipe[idx]['name'] != 'SaveData' and pipe[idx]['parameter']):\n",
    "            \n",
    "            # for param in pipe[idx]['parameter'][0]:\n",
    "            #     newkey = pipe[idx]['name']+'__'+ param\n",
    "            #     param_pipeline[newkey] = pipe[idx]['parameter'][0][param]\n",
    "\n",
    "\n",
    "            strp = pipe[idx]['name']+'()'\n",
    "            if check_fitted(eval(strp)):\n",
    "                for param in pipe[idx]['parameter'][0]:\n",
    "                    newkey = 'model__'+ param\n",
    "                    param_pipeline[newkey] = pipe[idx]['parameter'][0][param]\n",
    "            else:\n",
    "                for param in pipe[idx]['parameter'][0]:\n",
    "                    newkey = pipe[idx]['name']+'__'+ param\n",
    "                    param_pipeline[newkey] = pipe[idx]['parameter'][0][param]\n",
    "\n",
    "        elif(pipe[idx]['name'] != 'SaveData' and not pipe[idx]['parameter']):\n",
    "            # print('NOT', pipe[idx]['name'])\n",
    "            continue\n",
    "        else:\n",
    "            final_pipeline_param.append(param_pipeline)\n",
    "            param_pipeline = {}\n",
    "            continue\n",
    "\n",
    "    # print(final)\n",
    "    final_pipeline_param.append(param_pipeline)\n",
    "    return final_pipeline_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_param(raw_pipe, \"global-server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "619ca890e37fc0b5a7bf3970831144904e1971bb38cea205f0f5242315187158"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
